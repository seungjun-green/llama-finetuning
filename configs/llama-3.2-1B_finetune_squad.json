{
    "base_model_name": "meta-llama/Llama-3.2-1B",
    "lora_rank": 8,
    "lora_alpha": 16,
    "learning_rate": 2e-4,
    "batch_size": 16,
    "max_seq_length": 512,
    "num_epochs": 5,
    "output_dir": "./fine_tuned_checkpoints",
    "log_steps": 800,
    "device": "cuda",
    "use_fp16": true,
    "warmup_ratio": 0.1,
    "use_gradient_clipping": true,
    "max_grad_norm": 1,
    "patience": 5,
    "train_file_path": "",
    "dev_file_path": ""
}