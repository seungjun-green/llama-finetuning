# Llama Fine-Tuning on SQuAD Dataset

This repository provides a streamlined solution for fine-tuning Llama models on the SQuAD (Stanford Question Answering Dataset). The implementation is modular, enabling easy integration and usage.

---

## ğŸš€ Features

- Fine-tune Llama models efficiently.
- Designed for SQuAD Dataset out-of-the-box.
- Modular structure for seamless integration into your workflow.

---

## ğŸ“‹ How to Use This Repository

### 1. Clone the Repository

```bash
git clone https://github.com/seungjun-green/llama-finetuning.git
cd llama-finetuning
```

### 2. Import Required Modules

```python
import sys
sys.path.append("/content/llama-finetuning")
from models.base_model import load_base_model
from scripts.train import fine_tune
```

### 3. Load the Base Model and Tokenizer

```python
tokenizer, model = load_base_model("meta-llama/Llama-3.2-1B")
```

### 4. Fine-Tune the Model

```python
fine_tune(model,
          tokenizer,
          config_path='/content/config.json',
          train_file_path="/content/train-v1.1.json",
          dev_file_path="/content/dev-v1.1.json")
```

---

## ğŸ“‚ Directory Structure

```
llama-finetuning/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llama-3.2-1B_lora_finetune.json # Configuration for fine-tuning
â”‚   â”œâ”€â”€ squad_config.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ squad_data.py  # Data preprocessing for SQuAD
â”‚   â”œâ”€â”€ fine_tuned_checkpoints/ # Directory for storing checkpoints
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_model.py  # Code for loading base Llama models
â”‚   â”œâ”€â”€ lora.py        # LoRA (Low-Rank Adaptation) implementation
â”‚   â”œâ”€â”€ loss.py        # Loss functions for fine-tuning
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ eval.py        # Script for evaluating fine-tuned models
â”‚   â”œâ”€â”€ train.py       # Fine-tuning script
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ checkpoint.py  # Utilities for saving/loading checkpoints
â”‚   â”œâ”€â”€ helpers.py     # Helper functions
â”‚   â”œâ”€â”€ keys.py        # Key management for models and APIs
```

---

## ğŸ”® Future Plans

While the current implementation focuses on fine-tuning with the SQuAD Dataset, we plan to expand its capabilities, including:

- Support for additional datasets (e.g., text classification, summarization, etc.).
- Compatibility with other Llama model variants.
- Enhanced training utilities for distributed and large-scale fine-tuning.

---

## ğŸ›  Requirements

- Python 3.8+
- PyTorch
- Transformers Library
- SQuAD Dataset JSON files (`train-v1.1.json` and `dev-v1.1.json`)

Install dependencies using:

```bash
pip install -r requirements.txt
```

---

## ğŸ’¡ Example Use Case

Fine-tune a Llama model on the SQuAD dataset to create a question-answering system. The fine-tuned model can be used to:

- Answer questions from a given context.
- Serve as a foundation for building custom QA pipelines.

---

## ğŸ¤ Contributions

Contributions are welcome! Feel free to open an issue or submit a pull request for suggestions, bug fixes, or new features.

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ“¬ Contact

For questions or support, please reach out to [seungjun-green](https://github.com/seungjun-green).

---

Happy fine-tuning! ğŸ‰

